{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "https://docs.opencv.org/4.x/dd/d43/tutorial_py_video_display.html",
   "id": "8e4a6023476060ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Setup\n",
    "Import the necessary modules.\n",
    "\n",
    "# Import Model\n",
    "Import the external Hand Landmarker model asset by the path.\n",
    "Then create an options object for the landmarker parameters by using BaseOptions object, `model_asset_path`, set the running mode to Live Stream and a hand landmarker instance with the livestream mode.\n",
    "\n",
    "The resultListener must be called to set up a listener to receive results asynchronously.\n",
    "\n",
    "Create the HandLandmarker object with the options passed as a parameter\n",
    "\n",
    "# Load the Data\n",
    "The input media must first be converted into a mediapipe.Image object.\n",
    "For video or live stream, the frames need to be loaded as numpy arrays to be passed into the parameter.\n",
    "\n",
    "# Running the Hand Landmarker Process\n",
    "The Hand Landmarker preprocesses the data, detects hands in the image or frame, and detect the hand landmarks.\n",
    "For livestream, the image data is sent to the Hand Landmarker and results are accessible via the `result_callback` provided in the `HandLandmarkerOptions` object.\n",
    "\n",
    "# Displaying the results\n",
    "The Hand Landmarker returns hand landmarker result object `HandLandmarkerResult` for each detection run, containing:\n",
    "- hand landmark image coordinates\n",
    "- hand landmark world coordinates\n",
    "- left/right hand marker\n",
    "\n",
    "Each of the 21 hand landmarks are in `(x, y, z)` coordinates.\n",
    " `x` and `y` are normalized as width, height: `[0.0, 1.0]`.\n",
    " `z` is the landmark depth from the origin (0, 0) which starts at the wrist. The magnitude is the closeness to the camera.\n",
    " \n",
    "`result_callback`\tSets the result listener to receive the detection results asynchronously when the hand landmarker is in live stream mode. Only applicable when running mode is set to LIVE_STREAM. The results are only available in this callback.\n",
    "\n",
    "As coordinates obtained from hand_landmarks are normalized [0,1], they need to be multiplied by the input image's original width and height to display on the correct coordinates on the output frame/image.\n"
   ],
   "id": "2e948b5aeb39e41a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T01:38:32.970323Z",
     "start_time": "2024-04-22T01:38:31.629170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import cv2 as cv"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T01:38:32.975322Z",
     "start_time": "2024-04-22T01:38:32.972065Z"
    }
   },
   "cell_type": "code",
   "source": "model_path = 'hand_landmarker.task'",
   "id": "77e5efb9bea02c8e",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T01:38:32.979958Z",
     "start_time": "2024-04-22T01:38:32.976932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "BaseOptions = mp.tasks.BaseOptions\n",
    "HandLandmarkerOptions = vision.HandLandmarkerOptions\n",
    "HandLandmarkerResult = vision.HandLandmarkerResult\n",
    "VisionRunningMode = vision.RunningMode"
   ],
   "id": "ff90ce2b62d4e066",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T01:38:33.029771Z",
     "start_time": "2024-04-22T01:38:32.982545Z"
    }
   },
   "cell_type": "code",
   "source": [
    "options = HandLandmarkerOptions(base_options=BaseOptions(model_asset_path=model_path),\n",
    "                                running_mode=VisionRunningMode.IMAGE, num_hands=2)\n",
    "HandLandmarker = vision.HandLandmarker.create_from_options(options)"
   ],
   "id": "95c31cf2273e8fb3",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T01:38:33.041830Z",
     "start_time": "2024-04-22T01:38:33.031905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import numpy as np\n",
    "\n",
    "MARGIN = 10  # pixels\n",
    "FONT_SIZE = 1\n",
    "FONT_THICKNESS = 1\n",
    "HANDEDNESS_TEXT_COLOR = (88, 205, 54) # vibrant green\n",
    "\n",
    "def print_result(result: HandLandmarkerResult, output_image: mp.Image, timestamp_ms: int):\n",
    "    print('hand landmarker result: {}'.format(result))\n",
    "\n",
    "def draw_landmarks(current_frame, landmarker_result):\n",
    "  hand_landmarks_list = landmarker_result.hand_landmarks\n",
    "  handedness_list = landmarker_result.handedness\n",
    "  annotated_image = np.copy(current_frame)\n",
    "\n",
    "  # Loop through the detected hands to visualize.\n",
    "  for idx in range(len(hand_landmarks_list)):\n",
    "    hand_landmarks = hand_landmarks_list[idx]\n",
    "    handedness = handedness_list[idx]\n",
    "\n",
    "    # Draw the hand landmarks.\n",
    "    hand_landmarks_protocol = landmark_pb2.NormalizedLandmarkList()\n",
    "    hand_landmarks_protocol.landmark.extend([\n",
    "    # Normalize the coordinates\n",
    "      landmark_pb2.NormalizedLandmark(\n",
    "          x=landmark.x, \n",
    "          y=landmark.y, \n",
    "          z=landmark.z) for landmark in hand_landmarks ])\n",
    "    \n",
    "    # Draw the landmarks\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "      annotated_image,\n",
    "      hand_landmarks_protocol,\n",
    "      solutions.hands.HAND_CONNECTIONS,\n",
    "      solutions.drawing_styles.get_default_hand_landmarks_style(),\n",
    "      solutions.drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "    # Get the top left corner of the detected hand's bounding box.\n",
    "    height, width, _ = annotated_image.shape\n",
    "    x_coordinates = [landmark.x for landmark in hand_landmarks]\n",
    "    y_coordinates = [landmark.y for landmark in hand_landmarks]\n",
    "    text_x = int(min(x_coordinates) * width)\n",
    "    text_y = int(min(y_coordinates) * height) - MARGIN\n",
    "    \n",
    "    # Draw bounding box\n",
    "    xmin, xmax = int(min(x_coordinates) * width), int(max(x_coordinates) * width)\n",
    "    ymin, ymax = int(min(y_coordinates) * height), int(max(y_coordinates) * height)\n",
    "    boxW, boxH = xmax - xmin, ymax - ymin\n",
    "    cv.rectangle(annotated_image, \n",
    "                 pt1=(xmin - 20, ymin - 20), pt2=(xmin + boxW + 20, ymin + boxH + 20), \n",
    "                 color=(0, 0, 255), thickness=2)\n",
    "    \n",
    "    # Draw handedness (left or right hand) on the image.\n",
    "    cv.putText(annotated_image, f\"{handedness[0].category_name}\", (text_x, text_y), cv.FONT_HERSHEY_DUPLEX, FONT_SIZE, HANDEDNESS_TEXT_COLOR, FONT_THICKNESS, cv.LINE_AA)\n",
    "               \n",
    "  return annotated_image"
   ],
   "id": "606941893dc2e80b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-04-22T01:38:33.045157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create Video Capture Object\n",
    "# Arg as the device index (0 or -1) or name of a video file\n",
    "capture = cv.VideoCapture(0) \n",
    "if not capture.isOpened():\n",
    "    print(\"Cannot open camera\")\n",
    "    exit()\n",
    "    \n",
    "while True:\n",
    "    ret, current_frame = capture.read()\n",
    "    #ret = cap.set(cv.CAP_PROP_FRAME_WIDTH,320)\n",
    "    #ret = cap.set(cv.CAP_PROP_FRAME_HEIGHT,240)\n",
    "\n",
    "    # if frame is read correctly ret is True\n",
    "    if not ret: \n",
    "        print(\"Can't receive frame (stream end?). Exiting ...\") \n",
    "        break\n",
    "    current_frame = cv.flip(current_frame, 1)\n",
    "    gray = cv.cvtColor(current_frame, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Convert the frame received from OpenCV to a MediaPipeâ€™s Image object.\n",
    "    mp_frame = mp.Image(image_format=mp.ImageFormat.SRGB, data=current_frame)\n",
    "    \n",
    "    HandLandmarkerResult = HandLandmarker.detect(mp_frame)\n",
    "    annotated_frame = draw_landmarks(mp_frame.numpy_view(), HandLandmarkerResult)\n",
    "    \n",
    "    cv.imshow('annotated_frame', cv.cvtColor(annotated_frame, cv.COLOR_BGR2RGB))\n",
    "    \n",
    "    if cv.waitKey(1) == ord('q'): \n",
    "        break\n",
    "        # When everything done, release the capture\n",
    "        capture.release()\n",
    "        cv.destroyAllWindows()"
   ],
   "id": "71489046262ffed2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "5b33cbd5448750cb",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
